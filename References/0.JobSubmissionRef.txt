=====================================================================================================================================================================================
References
=====================================================================================================================================================================================
1. SLURM Job Scheduler: https://slurm.schedmd.com/documentation.html
                        https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html 
   Tutorial:            https://www.youtube.com/watch?v=hFsPY0Ti1gE&t=3211s&ab_channel=CECIandCISMHPC 

2. PBS Job Scheduler (Portable Batch System): https://2021.help.altair.com/2021.1.2/PBS%20Professional/PBSUserGuide2021.1.2.pdf  






















=====================================================================================================================================================================================
Creating a SBATCH Script
=====================================================================================================================================================================================
#!/usr/bin/env bash

#SBATCH --job-name=appSK
#SBATCH --partition=partitionName
#SBATCH --output=outputs/result-%j.out       
#SBATCH --error=outputs/result-%j.err
#SBATCH --mail-type=FAIL,END
#SBATCH --mail-user=sumant.kalra@hpe.com
#SBATCH --time=10:00:00

* Running 'a' multi-processing/multi-threaded application with SLURM (the common use case)
    #SBATCH --ntasks=1                              // Number of processes (ranks/pes) launched by the application 
    #SBATCH --cpus-per-task=2                       // Cores required per process 
    #SBATCH --ntasks-per-node=1                     // Processes on each node
    #SBATCH --mem-per-cpu=1g                        // Memory per core  

* Running multiple single-process (single/multi-threaded) applications with SLURM 
    #SBATCH --nodes=1                               // Number of nodes on which the job will span; does not allocate all the resources of the node  
    #SBATCH --ntasks-per-node=16                    // Processes on each node
    #SBATCH --cpus-per-task=1                       // Cores required per process
    #SBATCH --mem=2g                                // Memory per node        

* Important variables: 
    * ${SLURM_CPUS_PER_TASK}    : set by --cpus-per-task
    * ${SLURM_TASKS_PER_NODE}   : set by --tasks-per-node 
    * ${SLURM_SUBMIT_DIR}       : working directory on which sbatch is submitted
    * ${SLURM_ARRAY_TASK_ID}    : job index in a job array 

* Other important parameters:
   --exclusive                                     // Allocated nodes (cores of the nodes) are not shared with other jobs but the memory allocated is only as per request 
   --exclusive --mem=0                             // exclusive + all the memory on a node
   
   --time=<duration>                               // Refer to the documentation for the duration format
   
   --gpus=N                                        // N GPUs for the job 
   --gpus=Teslav100:1                              // 1 specific GPU for the job
   --gres=gpu:N                                    // N GPUs per allocated node 
   --gres=gpu:Teslav100:1                          // 1 specific GPU per allocated Node
   
   --constraints                                   // choose a specific feature (e.g. a processor type or a network type)
   --license                                       // access a specific licensed software 
   --partition                                     // choose a partition
   --test-only                                     // Enquire when the job will start 

   --comment="Something that I want"               // Attaches a comment to the job





























=====================================================================================================================================================================================
Monitor and Inspect the submitted jobs in queue
=====================================================================================================================================================================================
squeue                                                  // to show the job queue 
squeue --me                                             // show job queue for just your jobs
squeue -j <JOBID>                                       // to get information on a specific job
scontrol show job <JOBID>                               // job information which is more verbose
squeue --start -j <JOBID>                               // to get an estimate for when a job will run
jobperf <JOBID>                                         // instantaneous view of the cpu+memory usage of a running jobâ€™s nodes
scancel <JOBID>                                         // to cancel the job
scancel -u USERID                                       // to cancel all your jobs (careful!)
sinfo -p compute                                        // to look at available nodes
sacct                                                   // to get information on your recent jobs






[PENDING OR RUNNING]
(a) squeue : Information about the jobs in slurm scheduling queue [PENDING OR RUNNING] 
    squeue                                                                          // All the submitted jobs 
    squeue --me                                                                     // Submitted by me
    squeue --me --start                                                             // Also displays the expected start time of the jobs in pending status (PD) 
    squeue --partition=debug                                                        // Filter the jobs with partition=debug 
    squeue --Format=jobid,partition,timeused,timelimit --partition=debug            // Format the squeue output + filter partition=debug

[RUNNING]
(b) sstat : List status info for a currently running job [RUNNING]
    sstat --format=AveCPU,AvePages,AveRSS,AveVMSize,JobID -j <jobid> --allsteps     // Formatted status info for a job = jobId  

[PENDING OR RUNNING OR COMPLETED]
(c) scontrol show : Detailed information about the job [PENDING OR RUNNING OR COMPLETED] 
    scontrol show jobid -dd <jobId> 

[RUNNING or COMPLETED]
(d) sacct : Statistics about the job [RUNNING or COMPLETED]; Useful for completed job to obtain information like run-time, memory used etc that was not available during the run.
    sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed                          // Formatted statistics for a job = jobId
    sacct -u <username> --format=JobID,JobName,MaxRSS,Elapsed                       // Formatted statistics of all the jobs submitted by user = username 

    Note: Details on the completed jobs are available in slurm database only for a limited peroid of time.

[Control your jobs]
    scontrol show <jobId>                                                               // Show the details of the job with <jobId>
    scontrol update jobid=<jobId> <parameter>=<value>                                   // Modify a job in Pending state 
    scancel <jobId>                                                                     // cancel the job with <jobId> 
    scancel -n <jobName>                                                                // cancel the job with <jobName>

[Discover the cluster features]
    sinfo 
    sinfo "%4D %9P %25f %.5c %.8m %G"
    sacctmgr list qos                                                                   // QOS: Quality of service: used by sysadmin to organize/prioritize jobs
    scontrol show licenses                                                              // License: used to organize software license distribution to jobs 



































=====================================================================================================================================================================================
1. Embarrassingly Parallel Problem on HPC; Characteristics
2. The Scheduler; Job; Functionality
3. Running 'a' multi-processing/multi-threaded application with SLURM (the common use case) 
    WRITE; SUBMIT; ImportantPoints(a)srun(b)MaxResources(c)Home_vs_Scratch_Dir
4. Running multiple single-process (single/multi-threaded) applications with SLURM 
    JobArray; BackgroundProcessesWithinJob; BackgroundProcessesWithinJob-Batches; GNU_Parallel
5. RAMdisk
6. Profiling with Slurm  
=====================================================================================================================================================================================

=====================================================================================================================================================================================
1. Problem: Run an "Embarrassingly Parallel Problem" on HPC cluster
    Characteristics of an Embarrassingly Parallel Problem:
        - contains a large number of independent tasks that can run in parallel.
        - the independent tasks are serial in nature. 
        - little to no coordination/communication required among the parallel tasks (hence no need to create it as an MPI application)

2. The Scheduler 
    (a) The scheduler implements a batch operating system for the usage of computing resources on HPC. Example - Slurm
    (b) Job = 'Resource Request' + 'commandsToBeExecuted' 
                (Do not forget to set up the environment for e.g. by loading the appropriate modules before executing them from the script)
    (c) Functions performed:
        - provides the requested computing resources (cores and memory) from the HPC cluster by organizing the workload on the cluster, 
        - schedules the various job submitted for execution on cluster minimizing the idle time.
        - uses complicated algorithms that considers many variables like available resources, user's priority, times and resources requested for scheduling.

3. Running 'a' multi-processing/multi-threaded application with SLURM (the common use case)
   3.1 WRITE:  
    (a) Prerequisite: Identify the following aspects about the program to be executed on the cluster.
        1. Number of processes it runs                  --->            --ntasks
        2. Number of threads in each process            --->            --cpus-per-task 
        3. Memory required by each process              --->            --mem-per-cpu
        4. Control distribution of processes on nodes   --->            --ntasks-per-node  
        Other aspects can also be identified like gpus required etc depending upon the program.

    (b) Write a submission script scriptName.sh
        1. Shebang
            #!/usr/bin/env bash                                          
        2. Slurm directives 
            #SBATCH --job-name=test 
            #SBATCH --output=res.txt 
            #SBATCH --partition=debug 
            #SBATCH --time=00:10:00
            #SBATCH --ntasks=1
            #SBATCH --cpus-per-task=2
            #SBATCH --ntasks-per-node=1
            #SBATCH --mem-per-cpu=1g

            NOTE: 
            (a) No bash variables are allowed in the parameters.
            (b) No bash commands are allowed in between any two bash parameters.
            (c) Job parameters can be specified by: #SBATCH directives in submission script; Environment Variable; Command line parameters from with sbatch // (Right overrides Left)
        3. Setup the environment (Must load the module used in building to get the required run time libraries)
            module load OpenMPI 
        4. Commands to be executed 
            ./application 
   
    3.2 SUBMIT: 
    (a) Test the script first before submitting it as a job.
        - Execute the script as a shell script on the login node instead of submitting it with sbatch to cluster or request an interactive debug job on cluster using srun
    (b) Submit the job if the test run is successful
         sbatch run.sh [commandLineParameters]   
        
    3.3 Important points:
    (a) srun (run parallel tasks)
        Two different modes of operation:
        1. Submitting the submission script: srun vs sbatch
            - srun executes in interactive and blocking mode that is the job is tied to the terminal submitting the job, if the SSH session is interrupted for any reason, the srun will automatically be cancelled.
            - sbatch executes in batch processing and non-blocking mode that is the job runs independent of the terminal and the SSH session, and the outputs are printed in a file.
            - srun (run parallel tasks) starts 'ntasks' parallel subjobs whereas sbatch just submits the job with the number of processes being controlled by mpirun or srun called from the script.
            - srun is mostly used to run immediate jobs but sbatch can be used for later execution of jobs.
            - srun can not run Job arrays but sbatch can.
        2. Inside the submission script: srun vs mpirun
            - srun starts the 'ntasks' number of processes for the command in the submission script and mpirun also executes the 'ntasks' number of processes for the command (if -n argument of mpirun is not used).
            - srun can replace mpirun if configured but NOT RECOMMENDED.  
          PREFER NOT TO USE IT UNTIL YOU ARE SURE ABOUT IT. 
    
        https://stackoverflow.com/questions/43767866/slurm-srun-vs-sbatch-and-their-parameters 
        https://stackoverflow.com/questions/51300165/any-use-case-for-mpirun-on-slurm-managed-cluster/51300814#51300814   
    (b) Why not request for the maximum possible resources from the cluster for any job?
        "More the resources requested, longer the job will wait in the queue." However, the execution of the job will be faster once the job runs.
    (c) HOME Vs SCRATCH Directories
        Generally,
         HOME   : ExpirationTime-Never; BackedUp-Yes; ReadWriteAccessOnLoginNode; ReadOnlyAccessOnComputeNode
         SCRATCH: ExpirationTime-Xdays; BackedUp-No;  ReadWriteAccessOnLoginNode; ReadWriteAccessOnComputeNode

4. Running multiple single-process (single/multi-threaded) applications with SLURM 
    * Processes do not need to communicate with each other 
    * Example:  Embarrassingly Parallel Problems such as parameters studies (same application run with different parameters)
    [Refer to 1.2.SerialJobs.pdf in HPC notes for complete details]
    
    (a) JobArray: Multiple jobs submission by a single sbatch script  
    (b) BackgroundProcessesWithinJob: Multiple independent parallel processes in a single job 
    (c) BackgroundProcessesWithinJob-Batches: Multiple independent parallel processes in a single job 
    [d] GNU Parallel

    (a) JobArray: 'N' number of jobs are submitted at once for scheduling by a single sbatch script 
        - copies the same job into 'N' distinct jobs while each job is scheduled separately.
        - Slurm parameter: --array=1-N 
        - Other #SBATCH parameters same as in the case of running 'a' multi-processing/multi-threaded application
        - Each distinct job has distinct value of $SLURM_ARRAY_TASK_ID 
        - The slurm parameters in the submission script are for each of the job.
        - LIMITATION: However, it is not suitable for a large number of 'SHORT' serial jobs (execution time of a few seconds) 
          because each subjob is scheduled separately that adds a scheduling overhead of minutes (~1 min) on each of the job.  
    
    // --------------------------------------------------------------------------------------------------- //
    
    #SBATCH --nodes=1               ## number of nodes on which the job will span; does not allocate all the resources of a node
    #SBATCH --ntasks-per-node=16    ## number of processes to be run simultaneously in a node 
    #SBATCH --cpus-per-task=1       ## number of cpus for each task
    #SBATCH --mem=2g                ## memory required per node. If 0, then all the memory of the node is allocated
    > Refer to the scripts in 1.2.SerialJobs.pdf in HPC notes for complete details on the sbatch script   
    
    (b) BackgroundProcessesWithinJob: 
        - All the 'N' background processes are started at once with a single job submission 
        - use 'wait' as the last command in the submission script otherwise the job will terminate as the control reaches the end of the script; 'wait' will keep the job running until all the background processes are done.
        - LIMITATION: Large number of tasks per node may not fit because of memory constraints of a node; 
          Options: 1. Requests for more nodes 
                   2. Batch processing if limited by the number of nodes 

    (c) Batches-BackgroundSubJobsWithinJob: 
        - 'N' background processes are started in batches of a size 'K' processes per batch in the job
        - typically: --ntasks-per-node=batchSize(K); set in ${SLURM_TASKS_PER_NODE}
        - LIMITATION: No load balancing: Cpus that finish first will remain idle until the last process in the batch finishes.
    
    (d) GNU Parallel: Scheduler within scheduler
        - if one of the cpu is free, it is assigned the process from the next batch 
        - works very well by filling all the gaps for a job with a large number of processes ~100
           parallel -j <numberOfTasksInEachBatch>   <<EOF
            -----
            ----- each line as a process ------
            -----
           EOF
        - the tasks can also be provided in a file and provides as input in the place of EOF
        - ALWAYS PREFERRED for batch processing as most of the times, the time taken by each process is unknown. 

5. RAMdisk - Local I/O
    [Refer to 1.2.SerialJobs.pdf for complete details]
    - A part of the RAM is configured to be used as secondary memory for I/O
    - Much faster than directly writing to the secondary memory (SDD/HDD)
    - Useful with jobs involving heavy I/O, for e.g. each of the embarrassingly parallel problem's processes, say 1000 processes, writing unavoidably to files
    Important Points:
    - HPC cluster must be configured to use this feature.
    - RAMdisk = a location say /dev/shm/ is configured and should be used as 'working directory';
    - data needs to be staged in/out through the submission script; that is inside the submission script:
      > mkdir /dev/shm/$USER/workdir
      > cp $SLURM_SUBMIT_DIR/* /dev/shm/$USER/workdir
      > cd /dev/shm/$USER/workdir
      > <run subjobs>
      > <copy data out>
        tar cf $SLURM_SUBMIT_DIR/out.tar *.out
    - A part of the RAM is sacrificed
    
6. Profiling with Slurm - for optimization of resources to be requested for unknown programs 
    - Slurm can also be used for profiling using the information about running or completed jobs using sstat and sacct commands
    - Prefer profiling tools than the slurm commands for rigorous analysis 
    - Slurm commands for optimization of resources to be requested; 
      Particularly useful for programs for which nProcesses and nThreads are unknown.
        "More the resources requested, longer the job will wait in the queue"
        (a) Submit the job with some initial estimates of resources
        (b) Analyse the Feedback/Statistics from Slurm database using the commands 
        (c) Extrapolate for the next jobs
    Observations:
    1. top command: CPU Usage 100%      --> 1 core is fully utilized
                    CPU Usage n*100%    --> If n is a positive integer, the n cores are fully utilized else if n is a fraction then context switching 






















----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[EXTRAS]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SLURM Job Scheduler (Old Notes):
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 
sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l20 -C rhel7 -n 2 --nodes=1 runMysql20.sh

sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l20 -C rhel7 -n 4 --nodes=1 runMigrationScript20.sh

sbatch --parsable -w amsdc2-n-cp011a38 -A cldrn -C rhel7 -n 2 --nodes=1 runMysql.sh
sbatch --parsable -w amsdc2-n-cp011a38 -A cldrn -C rhel7 -n 2 --nodes=1 runMigrationScript.sh

squeue -u <userID>
squeue --me
squeue -j <JobID>
scancel <JobID>

scontrol show jobid -dd <jobid>

sacct -X --format="JobID,JobName%60,nodelist%30" -u <userId> -s R
																-s : Status
																R : Run
																PD : Pending
squeue -o"%.7i %.9P %.8j %.8u %.2t %.10M %.6D %C"

sacct -X --format="nodelist%30" | grep 

sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l21 -C rhel7 -n 4 --nodes=1 runMigrationScriptTest.sh

=============================================================================
References

LSF to SLURM:
https://help.jasmin.ac.uk/article/4891-lsf-to-slurm-quick-reference
https://elwe.rhrk.uni-kl.de/elwetritsch/slurm_guide.shtml

https://slurm.schedmd.com/documentation.html

https://stackoverflow.com/questions/43767866/slurm-srun-vs-sbatch-and-their-parameters

https://www.rit.edu/researchcomputing/instructions/Slurm-Basic-Commands
https://hpc.llnl.gov/banks-jobs/running-jobs/slurm-commands



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2. PBS Job Scheduler: 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. Submitting an executable as a job
    echo <executableFile> | qsub            # executableFile can be a shell command or any other application created by developer 
                                            # NOTE THAT MANY OTHER SCHEDULERS LIKE SLURM DONOT ALLOW TO SUBMIT AN EXECUTABLE FILE DIRECTLY TO THE SCHEDULER 
                                            # IN THOSE CASES, THE PROGRAM MUST BE CALLED FROM A BASH SCRIPT AND THE BATCH SCRIPT IN SUBMITTED TO THE SCHEDULER

## NOTE : MUST HAVE AN EMPTY LINE AT THE END OF THE BATCH SCRIPT OTHERWISE THE LAST COMMAND WILL NOT BE EXECUTED
2. Submit a Batch script / Submit script / Shell Script (Bash by default) (Preferred as it provides much more flexibility and the script can be used as metadata)
    qsub <script.sh>                        # script.sh is processed by shell (bash by default)
                                            # may contains PBS directives in lines beginning with #PBS

3. Querying Jobs
    qstat                                   
    qstat -f <JobId>                        # 567.c008; JobId=567, ServerName=c008 

4. Deleting Jobs  
    qdel <JobId>
    qdel all                                # Delete all the jobs run by you    

5. Sample PBS Script : https://github.com/sumant-kalra/HPC_Optimization/blob/main/Notes/pbs-QuickGuide.pdf (Refer to the doc for all the details) 

    #!/usr/bin/env bash 

    ### Job Name 
    #PBS -N hello_world_job

    ### Output files 
    #PBS -o hello_world_job.stdout
    #PBS -e hello_world_job.stderr 

    ### Changing Working Directory : If omitted then the home directory is used. Sets the environment variable PBS_O_INITDIR 
    #PBS -d ~/MyDir/Project 

    ### Comma separated list of desired resources : nodes; walltime; cput; mem; ncpus
    #PBS -l nodes=2,walltime=00:30:00,cput=00:10:00,ncpus=4,mem=4gb

    cat $PBS_NODEFILE


